{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Crawler-Updater\n",
    "\n",
    "This script will go into a folder of your choice and update the data in all the csv files in that folder. Mainly it will check for certain conditions in rows of certain columns and delete rows.\n",
    "\n",
    "## What it does\n",
    "1. Analyse csv files in a certain 'input folder':\n",
    "    * a) if they dont match a given structure those csv files will be moved to a 'bad column structure' folder.\n",
    "    * b) if they match the structure they will stay in the 'input folder'.\n",
    "2. For each csv file:\n",
    "    * a) Read the csv file into a new 'processed' file and then delete rows that don't contain any data.\n",
    "    * b) Count the number of rows.\n",
    "    * c) Count and delete 'Quality Assurance (QA)' Rows from the processed file. Note: QA rows are defined as rows without anything in the 'LOCATIONCODE' column AND they contain EITHER 'Blind Dup A' OR 'Field Blank' in the 'LocationDescription' column.\n",
    "    * d) Create a new file (NoSPT file) from the 'processed file' leaving only the rows without SPT codes. Remove the rows without SPT codes from the 'processed' file. A csv with the original filename with appended tag '-NoSPT' is created for the data with this issue.\n",
    "    * e) Create a new file (Retests file) from the 'processed file' leaving only the rows that are retests. Note: retests are defined as rows with the same location, date-time, and test methods; but have a different result. Remove the rows that are retests from the 'processed' file. A csv with the original filename with appended tag '-Retests' is created for the data with this issue.\n",
    "    * f) Create an Excel file for the counts of issues in each file that is analysed.\n",
    "    \n",
    "## How to use it\n",
    "1. Create folders for:\n",
    "    * a) the input file where all the csv's you wish to process live (Input_path_CSVs)\n",
    "    * b) Output file where all the csv's with unreadable column stuctures will be placed (Output_path_bad_structure)\n",
    "    * c) Output file where all the csv's  with only retests will be placed (Output_path_processed_csv)\n",
    "    * d) Output file where all the csv's  without SPT codes will be placed (Output_path_processed_csv)\n",
    "    * e) Output file where all the processed csv's will be placed (Output_path_processed_csv)\n",
    "2. Set the file paths for 1) under 'Input Variables' cell below. Also choose the filepath where you want the generated Report to be placed (Output_path_Report)\n",
    "3. Set the delimiter 'Input Variables'. Generally csv files use a comma (,) but I have it set to pipe (|) as default\n",
    "4. Run the code and wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of the csv files you want to process\n",
    "Input_path_CSVs = 'D:/FILES/Input_CSV/'\n",
    "\n",
    "# Can change to xlsx if needed, other changes will be nessesary to code\n",
    "Extension = 'csv'\n",
    "\n",
    "# Csv files seperator for input and output files..generally (,) or (|)\n",
    "Delimiter = '|'\n",
    "\n",
    "# Output of the CSV files\n",
    "Output_path_processed_csv = 'D:/FILES/Output_CSV_Processed/'\n",
    "\n",
    "# Output path of bad SPT CSV files\n",
    "Output_path_badSPT = 'D:/FILES/Output_CSV_Bad_SPT/'\n",
    "\n",
    "# Output path of Retest CSV files\n",
    "Output_path_Retests = 'D:/FILES/Output_CSV_Retests/'\n",
    "\n",
    "# Output path of CSV Files with Structure that can't be Analysed\n",
    "Output_path_bad_structure = 'D:/FILES/Output_CSV_Bad_Column_Structure/'\n",
    "\n",
    "# Output path of Report on Analysed files\n",
    "Output_path_Report = 'D:/FILES/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save CSV Filenames in a List and Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the csv filenames into an array\n",
    "os.chdir(Input_path_CSVs)\n",
    "filenames = [i for i in glob.glob('*.{}'.format(Extension))]\n",
    "\n",
    "# Get the number of csv files\n",
    "NumFiles = len(filenames)\n",
    "print(NumFiles, 'files will be processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find which files dont have 'LOCATIONCODE' or 'LocationDescription' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_good_files = 0\n",
    "counter_bad_files = 0\n",
    "List_Unsupported_Files = []\n",
    "for filename in filenames:\n",
    "    # Save an individual file as a DataFrame Object to analyse\n",
    "    try:\n",
    "        df_file = pd.read_csv(filename, sep=Delimiter, index_col=False, engine='python')\n",
    "        if ('LOCATIONCODE' in df_file.columns) and ('LocationDescription' in df_file.columns):\n",
    "            counter_good_files +=1\n",
    "        else:\n",
    "            List_Unsupported_Files.append(filename)\n",
    "            counter_bad_files +=1\n",
    "    except:\n",
    "        List_Unsupported_Files.append(filename)\n",
    "        counter_bad_files +=1\n",
    "        \n",
    "print('Number of Files that can be Analysed:')\n",
    "print(counter_good_files)\n",
    "print(\"Number of Files that can't be Analysed:\")\n",
    "print(counter_bad_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move Files with Weird Column Structure to Weird Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "files = os.listdir(Input_path_CSVs)\n",
    "\n",
    "for f in files:\n",
    "    if f in List_Unsupported_Files:\n",
    "        shutil.move(f, Output_path_bad_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop though each file and Process it, Create Excel Report on Changes Made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the csv filenames into an array\n",
    "os.chdir(Input_path_CSVs)\n",
    "filenames = [i for i in glob.glob('*.{}'.format(Extension))]\n",
    "\n",
    "#Create an Empty Dataframe for Report Numbers\n",
    "List_Columns = ['Filename', 'Total Rows', 'Duplicates', 'Retests', 'QA Data', 'No SPT Code']\n",
    "df_Report = pd.DataFrame(columns=List_Columns)\n",
    "df_Report.head()\n",
    "\n",
    "\n",
    "###################  START BIG LOOP  #######################################\n",
    "# Loop through each csv file in the input directory\n",
    "for filename in filenames:\n",
    "    # Set File Identifiers to Not Guilty untill proven guilty\n",
    "    QA_Data_In_File = False\n",
    "    Bad_sptz = False\n",
    "    Retests_In_File = False\n",
    "    Duplicates_In_File = False\n",
    "    \n",
    "    Int_Total_Rows = int(0)\n",
    "    Int_Bad_SPTz = int(0)\n",
    "    Int_QA_Rows = int(0)\n",
    "    Int_Dup_Rows = int(0)\n",
    "    Int_Retest_Rows = int(0)\n",
    "    \n",
    "    # Save an individual file as a DataFrame Object to analyse\n",
    "    df_file = pd.read_csv(filename, sep=Delimiter, index_col=False, engine='python')\n",
    "    # Delete Rows with everything missing in the row\n",
    "    df_file = df_file.dropna(axis='index', how='all')\n",
    "    Int_Total_Rows = df_file.shape[0]\n",
    "\n",
    "    \n",
    "    \n",
    "    ###################  QA DATA  ############################\n",
    "    # Check and Find if Blanks exist in Location Code Rows\n",
    "    bools_ml = df_file['LOCATIONCODE'].isnull()\n",
    "    bools_ml = np.array(bools_ml)\n",
    "\n",
    "    # Check and Find if Blanks exist in Location Description Rows\n",
    "    bools_md = df_file['LocationDescription'].isnull()\n",
    "    bools_md = np.array(bools_ml)\n",
    "\n",
    "    # Find Quality Assurance Data Rows\n",
    "    Series_Loc_Desc = df_file['LocationDescription']\n",
    "    Series_Loc_Desc = Series_Loc_Desc.fillna(' ')\n",
    "    bools_bd = Series_Loc_Desc == 'Blind Dup A'\n",
    "    bools_bd = np.array(bools_bd)\n",
    "    bools_bd = np.logical_and(bools_ml, bools_bd)\n",
    "    bools_fb = Series_Loc_Desc == 'Field Blank'\n",
    "    bools_fb = np.array(bools_fb)\n",
    "    bools_fb = np.logical_and(bools_ml, bools_fb)\n",
    "    bools_qa = np.logical_or(bools_bd, bools_fb)    \n",
    "    bools_filter_QA = np.invert(bools_qa)\n",
    "    if False in bools_filter_QA:\n",
    "        QA_Data_In_File = True\n",
    "        \n",
    "    # Fixing QA Data and update DataFrame object if neccesary\n",
    "    if QA_Data_In_File == True:\n",
    "        # Number of QA Rows in data\n",
    "        Int_QA_Rows = np.sum(bools_qa)\n",
    "        # Filter Out Quality Control Data from DataFrame Object\n",
    "        df_file = df_file[bools_filter_QA]\n",
    "\n",
    "        \n",
    "    ###################  MISSING SPT  ############################\n",
    "    # Check if SPT's still don't exist in Location Code Rows\n",
    "    Array_Loc_Codes = bools_good_sptz = df_file['LOCATIONCODE']\n",
    "    # Create Array of Loc-Codes and Fill Blank Locations with a space so that we can check if it begins with SPT\n",
    "    Array_Loc_Codes = Array_Loc_Codes.fillna(' ')\n",
    "    # Generate Array of Booleans for rows that have SPT's\n",
    "    bools_good_sptz = Array_Loc_Codes.str.startswith('SPT')\n",
    "    bools_good_sptz = np.array(bools_good_sptz)\n",
    "\n",
    "    \n",
    "    # Save bad SPT codes in data to a new dataframe\n",
    "    if False in bools_good_sptz:\n",
    "        Bad_sptz = True\n",
    "        # Create dataframe for stuff that can't load\n",
    "        df_cant_load = df_file\n",
    "        bools_filter_badSPT = np.invert(bools_good_sptz)\n",
    "        # Count the number of Bad SPTz\n",
    "        Int_Bad_SPTz = np.sum(bools_filter_badSPT)\n",
    "        # Leave only Bad SPT data in 'df_cant_load' Dataframe\n",
    "        df_cant_load = df_cant_load[bools_filter_badSPT]        \n",
    "        # Filter Orginal Dataframe 'df_file' to delete SPT rows that won't load\n",
    "        df_file = df_file[bools_good_sptz]\n",
    "    \n",
    "    ###################  DUPLICATES  ############################\n",
    "    # Create Lists to Check for duplicates\n",
    "    List_Row_Key = []\n",
    "    for index, row in df_file.iterrows():\n",
    "        String_Row =  str(row['LOCATIONCODE']) + str(row['SAMPLEDATE']) + str(row['TEST_KEY_CODE']) + str(row['RESULT'])\n",
    "        # Append the row to the list of Rows to Check for Duplicates\n",
    "        List_Row_Key.append(String_Row)\n",
    "\n",
    "    # Create Array of Rows from List of Rows\n",
    "    Array_Raw_Items_Results = np.array(List_Row_Key)\n",
    "    \n",
    "    # Decide if there are any exact duplicates    \n",
    "    if len(np.unique(Array_Raw_Items_Results)) != len(Array_Raw_Items_Results):\n",
    "        Duplicates_In_File = True\n",
    "    \n",
    "    # Find the rows where the exact duplicates live\n",
    "    if Duplicates_In_File == True:\n",
    "        # Create a dictionary of duplicate checker keys vs numvber of duplicates\n",
    "        unique, counts = np.unique(Array_Raw_Items_Results, return_counts=True)\n",
    "        Dict_Unique_Counts = dict(zip(unique, counts))\n",
    "        #Create a list of keys for duplicates only\n",
    "        List_Duplicate_Keys = ([key for key, val in Dict_Unique_Counts.items() if val > 1])\n",
    "        # Find which Row Strings are exact duplicates and add their Index to a list\n",
    "        List_Row_Index_Duplicates = []\n",
    "        for key in List_Duplicate_Keys:\n",
    "            # Get the row index's of the duplicate Key\n",
    "            List_Indexes = [i for i, j in enumerate(List_Row_Key) if j == key]\n",
    "            # Remove Fist Duplicate from List because we let the first one load\n",
    "            List_Indexes.pop(0)\n",
    "            # Append the row index's to a list\n",
    "            List_Row_Index_Duplicates.extend(List_Indexes)\n",
    "        # Count the number of Duplicates that will be deleted for Report\n",
    "        Int_Dup_Rows = len(List_Row_Index_Duplicates)\n",
    "        # Print feedback on Rows that are being deleted\n",
    "        print('Deleting Duplicate Rows from:', filename)\n",
    "        print(List_Row_Index_Duplicates)\n",
    "        # Delete the rows that are duplicates from dataframe df_file\n",
    "        df_file = df_file.drop(df_file.index[List_Row_Index_Duplicates])\n",
    "        \n",
    "    ###################  RETESTS  ############################\n",
    "    # Create Lists to Check for Retests\n",
    "    List_Row_Key = []\n",
    "    for index, row in df_file.iterrows():\n",
    "        String_Row =  str(row['LOCATIONCODE']) + str(row['SAMPLEDATE']) + str(row['TEST_KEY_CODE'])\n",
    "        # Append the Row String Item to the list of Rows\n",
    "        List_Row_Key.append(String_Row)\n",
    "\n",
    "    # Create Array of Rows from List of Rows\n",
    "    Array_Raw_Items_Results = np.array(List_Row_Key)\n",
    "    # Decide if there are any Retests   \n",
    "    if len(np.unique(Array_Raw_Items_Results)) != len(Array_Raw_Items_Results):\n",
    "        Retests_In_File = True\n",
    "    \n",
    "    # Find the rows where the Retests live\n",
    "    if Retests_In_File == True:\n",
    "        # Create a dictionary of Row Strings to number of Retests\n",
    "        unique, counts = np.unique(Array_Raw_Items_Results, return_counts=True)\n",
    "        Dict_Unique_Counts = dict(zip(unique, counts))\n",
    "        #Create a list of keys for Retests only\n",
    "        List_Retest_Keys = ([key for key, val in Dict_Unique_Counts.items() if val > 1])\n",
    "        # Find which Row Indexes are exact duplicates and add their Index to a list\n",
    "        List_Row_Index_Retests = []\n",
    "        for key in List_Retest_Keys:\n",
    "            # Get the row index's of the duplicate Key\n",
    "            List_Indexes = [i for i, j in enumerate(List_Row_Key) if j == key]\n",
    "            # Remove Fist Duplicate from List because we let the first one load\n",
    "            List_Indexes.pop(0)\n",
    "            # Append the row index's to a list\n",
    "            List_Row_Index_Retests.extend(List_Indexes)\n",
    "        # Count the number of Retests\n",
    "        Int_Retest_Rows = len(List_Row_Index_Retests)\n",
    "        # Print feedback on Rows that are being deleted\n",
    "        print('Moving Retests from', filename)\n",
    "        print(List_Row_Index_Retests)\n",
    "        \n",
    "        # Generate bools for Retest Rows\n",
    "        Num_New_Rows = df_file.shape[0]\n",
    "        List_Bools_Retests = []\n",
    "        Counter = int(0)\n",
    "        for i in range(Num_New_Rows):\n",
    "            if Counter in List_Row_Index_Retests:\n",
    "                List_Bools_Retests.append(True)\n",
    "                Counter += 1\n",
    "            else:\n",
    "                List_Bools_Retests.append(False)\n",
    "                Counter += 1\n",
    "        \n",
    "        # Convert the list of Bools to a Numpy Array\n",
    "        Array_Bools_Retests = np.array(List_Bools_Retests)\n",
    "        \n",
    "        # Create a new dataframe 'df_Retests' for only Retests\n",
    "        df_Retests = df_file[Array_Bools_Retests]\n",
    "                \n",
    "        # Delete the rows that are Retests from dataframe df_file\n",
    "        df_file = df_file.drop(df_file.index[List_Row_Index_Retests])\n",
    "        \n",
    "        \n",
    "    ###################  REPORT UPDATING ############################\n",
    "    # Append Update the Report Dataframe\n",
    "    List_Row_Report = [filename, Int_Total_Rows, Int_Dup_Rows, Int_Retest_Rows, Int_QA_Rows, Int_Bad_SPTz]\n",
    "    df_Temp_Report = pd.DataFrame([List_Row_Report], columns=List_Columns)\n",
    "    df_Report = df_Report.append(df_Temp_Report, ignore_index=True)\n",
    "  \n",
    "    ###################  PROCESSED FILE  ############################\n",
    "    # Create New Processed File if 1 or more rows\n",
    "    if df_file.shape[0] > 0:\n",
    "        new_filename = filename[:-4] + '-processed' + filename[-4:]\n",
    "        Output_filename = Output_path_processed_csv + new_filename\n",
    "        df_file.to_csv(path_or_buf=Output_filename, sep='|', index=False)\n",
    "    \n",
    "    ###################  CREATE NO SPT FILE  ############################\n",
    "    # Create New Bad File for fixes that can't be made\n",
    "    if (Bad_sptz == True):\n",
    "        if df_cant_load.shape[0] > 0:\n",
    "            new_filename = filename[:-4] + '-NoSPTs' + filename[-4:]\n",
    "            Output_filename = Output_path_badSPT + new_filename\n",
    "            df_cant_load.to_csv(path_or_buf=Output_filename, sep='|', index=False)   \n",
    "            \n",
    "    ###################  CREATE RETESTS FILE  ############################\n",
    "    if (Retests_In_File == True):\n",
    "        if df_Retests.shape[0] > 0:\n",
    "            new_filename = filename[:-4] + '-Retests' + filename[-4:]\n",
    "            Output_filename = Output_path_Retests + new_filename\n",
    "            df_Retests.to_csv(path_or_buf=Output_filename, sep='|', index=False)\n",
    "    \n",
    "###################  END BIG LOOP  #######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################  Create Excel Report  ############################\n",
    "# Generate Excel Report\n",
    "Report_path_filename = Output_path_Report + 'Report.xlsx'\n",
    "writer = pd.ExcelWriter(Report_path_filename)\n",
    "df_Report.to_excel(writer,'Sheet1')\n",
    "writer.save()\n",
    "\n",
    "# View start of Report in Notebook\n",
    "df_Report.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
